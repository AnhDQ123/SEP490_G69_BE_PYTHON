import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer
from TorchCRF import CRF
from torch.utils.data import DataLoader, TensorDataset


# ========== Define BERT-CRF Model ==========
# L·ªõp fc √°nh x·∫° ƒë·∫ßu ra BERT sang kh√¥ng gian nh√£n.
class BERTCRFModel(nn.Module):
    def __init__(self, bert_model_name, num_labels):
        super(BERTCRFModel, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.hidden_size = self.bert.config.hidden_size
        self.fc = nn.Linear(self.hidden_size, num_labels)
        self.crf = CRF(num_labels)
    # D·ªØ li·ªáu ƒëi qua BERT, tr·∫£ v·ªÅ vector ng·ªØ nghƒ©a c·ªßa t·ª´ng t·ª´.
    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        emissions = self.fc(outputs.last_hidden_state)
        # N·∫øu c√≥ labels:
        # T√≠nh loss ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh.
        # N·∫øu kh√¥ng c√≥ labels:
        # D·ª± ƒëo√°n nh√£n t·ªët nh·∫•t b·∫±ng CRF.
        if labels is not None:
            loss = (-self.crf(emissions, labels, mask=attention_mask.byte()) / input_ids.size(0)).mean()
            return loss
        else:
            predictions = self.crf.viterbi_decode(emissions, mask=attention_mask.byte())
            return predictions


# ========== Load Tokenizer & Labels ==========
# Tokenizer c·ªßa BERT gi√∫p bi·∫øn ƒë·ªïi vƒÉn b·∫£n th√†nh s·ªë ƒë·ªÉ m√¥ h√¨nh c√≥ th·ªÉ x·ª≠ l√Ω.
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")


# Label Mapping
label2id = {
    "B-CATEGORY": 1, "I-CATEGORY": 2,
    "B-BRAND": 3, "I-BRAND": 4,
    "B-DISCOUNT": 5, "I-DISCOUNT": 6,
    "B-ATTRIBUTE": 7, "I-ATTRIBUTE": 8,
    "B-ORIGIN": 9, "I-ORIGIN": 10,
    "B-WEIGHT": 11, "I-WEIGHT": 12,
    "B-SIZE": 13, "I-SIZE": 14,
    "B-VOLUME": 15, "I-VOLUME": 16,
    "B-POWER": 17, "I-POWER": 18,
    "B-COUNT": 19, "I-COUNT": 20,  # Th√™m nh√£n COUNT v√†o danh s√°ch
    "B-GENDER": 21, "I-GENDER": 22,
    "O": 0  # Nh√£n 'O' d√†nh cho token kh√¥ng c√≥ nh√£n
}

# G√°n s·ªë cho t·ª´ng nh√£n, gi√∫p m√¥ h√¨nh h·ªçc v√† d·ª± ƒëo√°n d·ªÖ d√†ng h∆°n.
# T·∫°o √°nh x·∫° ng∆∞·ª£c (id2label) ƒë·ªÉ chuy·ªÉn t·ª´ s·ªë v·ªÅ nh√£n.
id2label = {v: k for k, v in label2id.items()}

# Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n (query) th√†nh input_ids v√† attention_mask.
# X·ª≠ l√Ω subword tokenization c·ªßa BERT ƒë·ªÉ ƒë·∫£m b·∫£o nh√£n (labels) kh·ªõp v·ªõi token.
# Tr·∫£ v·ªÅ tensor ch·ª©a input v√† nh√£n ƒë·ªÉ m√¥ h√¨nh hu·∫•n luy·ªán ho·∫∑c d·ª± ƒëo√°n.
def encode_query(query, labels, max_length=32):
    tokens = tokenizer(query, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")

    tokenized_words = tokenizer.tokenize(query)
    adjusted_labels = ["O"] * len(tokenized_words)

    word_idx = 0
    for idx, token in enumerate(tokenized_words):
        if token.startswith("##"):  # Token ph·ª• thu·ªôc v√†o t·ª´ tr∆∞·ªõc
            adjusted_labels[idx] = adjusted_labels[idx - 1]
        else:
            if word_idx < len(labels):
                adjusted_labels[idx] = labels[word_idx]
                word_idx += 1

    label_ids = [label2id[label] for label in adjusted_labels] + [0] * (max_length - len(adjusted_labels))

    return tokens["input_ids"], tokens["attention_mask"], torch.tensor(label_ids)
# Example Training Data
train_data = [
    # üçî ƒê·ªì ƒÉn nhanh
    ("g√† r√°n KFC khuy·∫øn m√£i", ["B-CATEGORY", "I-CATEGORY", "B-BRAND", "B-DISCOUNT"]),
    ("burger McDonald's si√™u ngon", ["B-CATEGORY", "B-BRAND", "B-ATTRIBUTE"]),
    ("pizza Domino gi·∫£m gi√°", ["B-CATEGORY", "B-BRAND", "B-DISCOUNT"]),
    ("g√† r√°n Lotteria khuy·∫øn m√£i", ["B-CATEGORY", "I-CATEGORY", "B-BRAND", "B-DISCOUNT"]),
    ("hamburger Burger King combo ∆∞u ƒë√£i", ["B-CATEGORY", "B-BRAND", "B-ATTRIBUTE", "B-DISCOUNT"]),
    ("khoai t√¢y chi√™n McDonald's c·ª° l·ªõn", ["B-CATEGORY", "B-BRAND", "B-SIZE"]),
    ("g√† s·ªët cay Texas Chicken size M", ["B-CATEGORY", "B-BRAND", "B-ATTRIBUTE", "B-SIZE"]),
    ("b√°nh m√¨ Subway g√† n∆∞·ªõng", ["B-CATEGORY", "B-BRAND", "B-ATTRIBUTE"]),
    ("C∆°m gi√° r·∫ª", ["B-CATEGORY", "B-DISCOUNT", "I-DISCOUNT"]),
    ("Ph·ªü gi·∫£m gi√°", ["B-CATEGORY", "B-DISCOUNT", "I-DISCOUNT"]),
    ("C∆°m rang gi√° r·∫ª", ["B-CATEGORY", "B-ATTRIBUTE", "B-DISCOUNT","I-DISCOUNT"]),
    ("G√† r√°n gi√° r·∫ª", ["B-CATEGORY", "B-ATTRIBUTE","B-DISCOUNT","I-DISCOUNT"]),
    ("G√† r√°n gi√° r·∫ª", ["B-CATEGORY", "B-ATTRIBUTE","B-DISCOUNT","I-DISCOUNT"]),
    ("pizza Domino gi·∫£m gi√°", ["B-CATEGORY", "B-BRAND", "B-DISCOUNT"]),
    ("pizza Domino gi·∫£m gi√°", ["B-CATEGORY", "B-BRAND", "B-DISCOUNT"]),
    ("pizza Domino gi·∫£m gi√°", ["B-CATEGORY", "B-BRAND", "B-DISCOUNT"]),


    # ü•§ ƒê·ªì u·ªëng nhanh
    ("tr√† s·ªØa Gongcha size M", ["B-CATEGORY", "B-BRAND", "B-SIZE"]),
    ("tr√† ƒë√†o Highlands th∆°m ngon", ["B-CATEGORY", "B-BRAND", "B-ATTRIBUTE"]),
    ("c√† ph√™ s·ªØa Highlands ƒë·∫≠m ƒë√†", ["B-CATEGORY", "B-BRAND", "B-ATTRIBUTE"]),
    ("tr√† xanh Starbucks gi·∫£m gi√°", ["B-CATEGORY", "B-BRAND", "B-DISCOUNT"]),
    ("tr√† chanh B·ª•i Ph·ªë ƒë·∫∑c bi·ªát", ["B-CATEGORY", "B-BRAND", "B-ATTRIBUTE"]),
    ("sinh t·ªë xo√†i Ph√∫c Long khuy·∫øn m√£i", ["B-CATEGORY", "B-BRAND", "B-DISCOUNT"]),

    # üêü Th·ª±c ph·∫©m t∆∞∆°i s·ªëng
    ("c√° h·ªìi Nauy t∆∞∆°i s·ªëng 500g", ["B-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE", "B-WEIGHT"]),
    ("th·ªãt b√≤ Kobe Nh·∫≠t B·∫£n 200g", ["B-CATEGORY", "B-ORIGIN", "B-ORIGIN", "B-WEIGHT"]),
    ("t√¥m h√πm Alaska nh·∫≠p kh·∫©u", ["B-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE"]),
    ("cua ho√†ng ƒë·∫ø Na Uy 1kg", ["B-CATEGORY", "B-ORIGIN", "B-WEIGHT"]),
    ("h√†u s·ªØa Ph√°p t∆∞∆°i ngon", ["B-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE"]),
    ("s√≤ huy·∫øt C√† Mau lo·∫°i 1", ["B-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE"]),
    ("c√° ch·∫Ωm Vi·ªát Nam 1.2kg", ["B-CATEGORY", "B-ORIGIN", "B-WEIGHT"]),

    # ü•© Th·ªãt, tr·ª©ng, gia c·∫ßm
    ("th·ªãt heo Ba Lan ƒë√¥ng l·∫°nh 1kg", ["B-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE", "B-WEIGHT"]),
    ("b√≤ M·ªπ phi l√™ m·ªÅm ngon", ["B-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE"]),
    ("th·ªãt g√† ta nguy√™n con 2kg", ["B-CATEGORY", "B-ATTRIBUTE", "B-WEIGHT"]),
    ("tr·ª©ng g√† CP h·ªôp 10 qu·∫£", ["B-CATEGORY", "B-BRAND", "B-COUNT"]),
    ("tr·ª©ng v·ªãt mu·ªëi h·ªôp 6 qu·∫£", ["B-CATEGORY", "B-ATTRIBUTE", "B-COUNT"]),

    # üçé Rau c·ªß, tr√°i c√¢y
    ("nho M·ªπ nh·∫≠p kh·∫©u", ["B-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE"]),
    ("b∆° s√°p ƒê√† L·∫°t lo·∫°i 1", ["B-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE"]),
    ("d√¢u t√¢y H√†n Qu·ªëc h·ªôp 500g", ["B-CATEGORY", "B-ORIGIN", "B-WEIGHT"]),
    ("xo√†i c√°t H√≤a L·ªôc lo·∫°i ngon", ["B-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE"]),
    ("chu·ªëi ti√™u Laba ƒê√† L·∫°t 1kg", ["B-CATEGORY", "B-ORIGIN", "B-WEIGHT"]),
    ("t√°o Envy New Zealand size l·ªõn", ["B-CATEGORY", "B-ORIGIN", "B-SIZE"]),

    # ü•¨ Rau xanh t∆∞∆°i s·ªëng
    ("rau c·∫£i ng·ªçt ƒê√† L·∫°t 500g", ["B-CATEGORY", "B-ORIGIN", "B-WEIGHT"]),
    ("x√† l√°ch M·ªπ h·ªØu c∆° 200g", ["B-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE", "B-WEIGHT"]),
    ("c√† chua bi ƒê√† L·∫°t 1kg", ["B-CATEGORY", "B-ORIGIN", "B-WEIGHT"]),
    ("·ªõt chu√¥ng ƒë·ªè H√†n Qu·ªëc 3 tr√°i", ["B-CATEGORY", "B-ORIGIN", "B-COUNT"]),
    ("h√†nh l√° Vi·ªát Nam 200g", ["B-CATEGORY", "B-ORIGIN", "B-WEIGHT"]),

    # üçö Th·ª±c ph·∫©m kh√¥
    ("m√¨ H·∫£o H·∫£o th√πng 30 g√≥i", ["B-CATEGORY", "B-BRAND", "B-WEIGHT"]),
    ("g·∫°o ST25 t√∫i 5kg", ["B-CATEGORY", "B-BRAND", "B-WEIGHT"]),
    ("b√∫n kh√¥ Ph√∫ Qu·ªëc 500g", ["B-CATEGORY", "B-ORIGIN", "B-WEIGHT"]),
    ("d·∫ßu ƒÉn Neptune chai 1L", ["B-CATEGORY", "B-BRAND", "B-VOLUME"]),
    ("n∆∞·ªõc m·∫Øm Nam Ng∆∞ 500ml", ["B-CATEGORY", "B-BRAND", "B-VOLUME"]),

    # üç™ Th·ª±c ph·∫©m ƒë√≥ng g√≥i
    ("b√°nh quy Oreo h·ªôp 300g", ["B-CATEGORY", "B-BRAND", "B-WEIGHT"]),
    ("k·∫πo d·∫ªo Haribo ƒê·ª©c 200g", ["B-CATEGORY", "B-BRAND", "B-ORIGIN", "B-WEIGHT"]),
    ("s√¥ c√¥ la KitKat Nh·∫≠t B·∫£n", ["B-CATEGORY", "B-BRAND", "B-ORIGIN"]),
    ("s·ªØa Ensure Gold h·ªôp 850g", ["B-CATEGORY", "B-BRAND", "B-WEIGHT"]),
    ("y·∫øn m·∫°ch Quaker M·ªπ 1kg", ["B-CATEGORY", "B-BRAND", "B-ORIGIN", "B-WEIGHT"]),
    ("ng≈© c·ªëc Calbee Nh·∫≠t B·∫£n", ["B-CATEGORY", "B-BRAND", "B-ORIGIN"]),

    # üç∑ ƒê·ªì u·ªëng ƒë√≥ng chai
    ("bia Heineken lon 330ml", ["B-CATEGORY", "B-BRAND", "B-VOLUME"]),
    ("n∆∞·ªõc √©p cam Tropicana M·ªπ 1L", ["B-CATEGORY", "B-BRAND", "B-ORIGIN", "B-VOLUME"]),
    ("tr√† xanh C2 chai 500ml", ["B-CATEGORY", "B-BRAND", "B-VOLUME"]),
    ("s·ªØa chua u·ªëng Yakult Nh·∫≠t B·∫£n", ["B-CATEGORY", "B-BRAND", "B-ORIGIN"]),

    ("Ph·ªü b√≤ Nam ƒê·ªãnh ngon nh·∫•t", ["B-CATEGORY", "B-ATTRIBUTE", "B-ORIGIN", "I-ORIGIN", "B-ATTRIBUTE"]),
    ("B√∫n ch·∫£ H√† N·ªôi ch√≠nh g·ªëc", ["B-CATEGORY", "I-CATEGORY", "B-ORIGIN", "B-ATTRIBUTE"]),
    ("M√¨ Qu·∫£ng ƒê√† N·∫µng ƒë·∫∑c s·∫£n", ["B-CATEGORY", "I-CATEGORY", "B-ORIGIN", "I-ORIGIN", "B-ATTRIBUTE"]),
    ("Ch√°o l∆∞∆°n Ngh·ªá An cay n·ªìng", ["B-CATEGORY", "I-CATEGORY", "B-ORIGIN", "I-ORIGIN", "B-ATTRIBUTE"]),
    ("Ph·ªü g√† H√† N·ªôi h∆∞∆°ng v·ªã truy·ªÅn th·ªëng", ["B-CATEGORY", "B-ATTRIBUTE", "B-ORIGIN", "B-ATTRIBUTE", "I-ATTRIBUTE"]),
    ("L·∫©u b√≤ S√†i G√≤n ƒë·∫≠m ƒë√†", ["B-CATEGORY", "B-ATTRIBUTE", "B-ORIGIN", "B-ATTRIBUTE"]),
    ("B√≤ H·∫ßm", ["B-CATEGORY", "B-ATTRIBUTE",]),
    ("G√† H·∫ßm Thu·ªëc B·∫Øc", ["B-CATEGORY", "B-ATTRIBUTE","I-ATTRIBUTE","I-ATTRIBUTE"]),
    ("C∆°m L∆∞∆°n s·ªët Teryaki", ["B-CATEGORY", "B-ATTRIBUTE","I-ATTRIBUTE","B-BRAND"]),
("C∆°m L∆∞∆°n s·ªët Teryaki", ["B-CATEGORY", "I-CATEGORY", "B-ATTRIBUTE", "B-BRAND"]),
("C∆°m L∆∞∆°n s·ªët cay", ["B-CATEGORY", "I-CATEGORY", "B-ATTRIBUTE"]),
("C∆°m rang L∆∞∆°n s·ªët ƒë·∫∑c bi·ªát", ["B-CATEGORY", "I-CATEGORY", "B-ATTRIBUTE", "B-ATTRIBUTE"])
]

# Prepare Data for Training
# train_inputs: M√£ h√≥a c·ªßa c√¢u (BERT token IDs).
# train_masks: Mask tensor ƒë·ªÉ ch·ªâ ƒë·ªãnh token n√†o l√† th·∫≠t (1), token n√†o l√† padding (0).
# train_labels: Nh√£n th·ª±c th·ªÉ c·ªßa t·ª´ng token trong c√¢u.
train_inputs, train_masks, train_labels = [], [], []

# L·∫∑p qua t·ª´ng c√¢u (query) v√† nh√£n th·ª±c th·ªÉ (label) trong t·∫≠p hu·∫•n luy·ªán (train_data).
# G·ªçi encode_query(query, label) ƒë·ªÉ chuy·ªÉn c√¢u th√†nh tensor:
# input_ids: Tokenized ID c·ªßa c√¢u.
# attention_mask: Mask tensor cho BERT.
# label_ids: Nh√£n th·ª±c th·ªÉ d·∫°ng s·ªë.
# Th√™m c√°c gi√° tr·ªã v√†o danh s√°ch train_inputs, train_masks, train_labels.
for query, label in train_data:
    input_ids, attention_mask, label_ids = encode_query(query, label)
    train_inputs.append(input_ids)
    train_masks.append(attention_mask)
    train_labels.append(label_ids)

# Chuy·ªÉn danh s√°ch th√†nh tensor
train_inputs = torch.cat(train_inputs, dim=0)
train_masks = torch.cat(train_masks, dim=0)
train_labels = torch.stack(train_labels, dim=0)

# Create DataLoader
batch_size = 8
train_dataset = TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Initialize Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BERTCRFModel("bert-base-uncased", num_labels=len(label2id)).to(device)
model_save_path = "bert_crf_model.pth"
torch.save(model.state_dict(), model_save_path)
print("M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng!")
# Load tr·ªçng s·ªë ƒë√£ l∆∞u tr∆∞·ªõc ƒë√≥
model_load_path = "bert_crf_model.pth"
model.load_state_dict(torch.load(model_load_path, map_location=device))
model.eval()  # ƒê·∫∑t m√¥ h√¨nh v√†o ch·∫ø ƒë·ªô ƒë√°nh gi√°
print("M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")
# Training Loop
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)  # Gi·∫£m LR xu·ªëng t·ª´ 5e-5
epochs = 10  # Hu·∫•n luy·ªán th√™m ƒë·ªÉ m√¥ h√¨nh h·ªçc k·ªπ h∆°n
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_dataloader:
        batch = [b.to(device) for b in batch]
        input_ids, attention_mask, labels = batch

        optimizer.zero_grad()
        loss = model(input_ids, attention_mask, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}")


# Prediction Function
def predict(query):
    model.eval()
    input_ids, attention_mask, _ = encode_query(query, ["O"] * len(query.split()))
    input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)

    with torch.no_grad():
        predictions = model(input_ids, attention_mask)

    predicted_labels = [id2label[label] for label in predictions[0]]
    return list(zip(query.split(), predicted_labels))


# Example Prediction
query = "g√† ri r√°n KFC khuy·∫øn m√£i"
print(predict(query))

