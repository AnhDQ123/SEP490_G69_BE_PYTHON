import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer
from TorchCRF import CRF
from torch.utils.data import DataLoader, TensorDataset
from flask import Flask, request, jsonify
import pandas as pd
import os
from torch.utils.data import TensorDataset, DataLoader
# ========== Define BERT-CRF Model ==========
# L·ªõp fc √°nh x·∫° ƒë·∫ßu ra BERT sang kh√¥ng gian nh√£n.
class BERTCRFModel(nn.Module):
    def __init__(self, bert_model_name, num_labels):
        super(BERTCRFModel, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.hidden_size = self.bert.config.hidden_size
        self.fc = nn.Linear(self.hidden_size, num_labels)
        self.crf = CRF(num_labels)
    # D·ªØ li·ªáu ƒëi qua BERT, tr·∫£ v·ªÅ vector ng·ªØ nghƒ©a c·ªßa t·ª´ng t·ª´.
    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        emissions = self.fc(outputs.last_hidden_state)
        # N·∫øu c√≥ labels:
        # T√≠nh loss ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh.
        # N·∫øu kh√¥ng c√≥ labels:
        # D·ª± ƒëo√°n nh√£n t·ªët nh·∫•t b·∫±ng CRF.
        if labels is not None:
            loss = (-self.crf(emissions, labels, mask=attention_mask.byte()) / input_ids.size(0)).mean()
            return loss
        else:
            predictions = self.crf.viterbi_decode(emissions, mask=attention_mask.byte())
            return predictions


# ========== Load Tokenizer & Labels ==========
# Tokenizer c·ªßa BERT gi√∫p bi·∫øn ƒë·ªïi vƒÉn b·∫£n th√†nh s·ªë ƒë·ªÉ m√¥ h√¨nh c√≥ th·ªÉ x·ª≠ l√Ω.
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")


# Label Mapping
label2id = {
    "B-CATEGORY": 1, "I-CATEGORY": 2,
    "B-BRAND": 3, "I-BRAND": 4,
    "B-DISCOUNT": 5, "I-DISCOUNT": 6,
    "B-ATTRIBUTE": 7, "I-ATTRIBUTE": 8,
    "B-ORIGIN": 9, "I-ORIGIN": 10,
    "B-WEIGHT": 11, "I-WEIGHT": 12,
    "B-SIZE": 13, "I-SIZE": 14,
    "B-VOLUME": 15, "I-VOLUME": 16,
    "B-POWER": 17, "I-POWER": 18,
    "B-COUNT": 19, "I-COUNT": 20,  # Th√™m nh√£n COUNT v√†o danh s√°ch
    "B-GENDER": 21, "I-GENDER": 22,
    "O": 0  # Nh√£n 'O' d√†nh cho token kh√¥ng c√≥ nh√£n
}

# G√°n s·ªë cho t·ª´ng nh√£n, gi√∫p m√¥ h√¨nh h·ªçc v√† d·ª± ƒëo√°n d·ªÖ d√†ng h∆°n.
# T·∫°o √°nh x·∫° ng∆∞·ª£c (id2label) ƒë·ªÉ chuy·ªÉn t·ª´ s·ªë v·ªÅ nh√£n.
id2label = {v: k for k, v in label2id.items()}

# Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n (query) th√†nh input_ids v√† attention_mask.
# X·ª≠ l√Ω subword tokenization c·ªßa BERT ƒë·ªÉ ƒë·∫£m b·∫£o nh√£n (labels) kh·ªõp v·ªõi token.
# Tr·∫£ v·ªÅ tensor ch·ª©a input v√† nh√£n ƒë·ªÉ m√¥ h√¨nh hu·∫•n luy·ªán ho·∫∑c d·ª± ƒëo√°n.
def encode_query(query, labels, max_length=32):
    tokens = tokenizer(query, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")

    tokenized_words = tokenizer.tokenize(query)
    adjusted_labels = ["O"] * len(tokenized_words)

    word_idx = 0
    for idx, token in enumerate(tokenized_words):
        if token.startswith("##"):  # Token ph·ª• thu·ªôc v√†o t·ª´ tr∆∞·ªõc
            adjusted_labels[idx] = adjusted_labels[idx - 1]
        else:
            if word_idx < len(labels):
                adjusted_labels[idx] = labels[word_idx]
                word_idx += 1

    label_ids = [label2id[label] for label in adjusted_labels] + [0] * (max_length - len(adjusted_labels))

    return tokens["input_ids"], tokens["attention_mask"], torch.tensor(label_ids)
# Example Training Data
df = pd.read_csv("training_data.csv")
train_data = []
for _, row in df.iterrows():
    query = row["query"]
    labels = row["labels"].split()
    train_data.append((query, labels))

# Prepare Data for Training
# train_inputs: M√£ h√≥a c·ªßa c√¢u (BERT token IDs).
# train_masks: Mask tensor ƒë·ªÉ ch·ªâ ƒë·ªãnh token n√†o l√† th·∫≠t (1), token n√†o l√† padding (0).
# train_labels: Nh√£n th·ª±c th·ªÉ c·ªßa t·ª´ng token trong c√¢u.
train_inputs, train_masks, train_labels = [], [], []

# L·∫∑p qua t·ª´ng c√¢u (query) v√† nh√£n th·ª±c th·ªÉ (label) trong t·∫≠p hu·∫•n luy·ªán (train_data).
# G·ªçi encode_query(query, label) ƒë·ªÉ chuy·ªÉn c√¢u th√†nh tensor:
# input_ids: Tokenized ID c·ªßa c√¢u.
# attention_mask: Mask tensor cho BERT.
# label_ids: Nh√£n th·ª±c th·ªÉ d·∫°ng s·ªë.
# Th√™m c√°c gi√° tr·ªã v√†o danh s√°ch train_inputs, train_masks, train_labels.
for query, label in train_data:
    input_ids, attention_mask, label_ids = encode_query(query, label)
    train_inputs.append(input_ids)
    train_masks.append(attention_mask)
    train_labels.append(label_ids)

# Chuy·ªÉn danh s√°ch th√†nh tensor
train_inputs = torch.cat(train_inputs, dim=0)
train_masks = torch.cat(train_masks, dim=0)
train_labels = torch.stack(train_labels, dim=0)

# Create DataLoader
batch_size = 8
train_dataset = TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BERTCRFModel("bert-base-uncased", num_labels=len(label2id)).to(device)

model_path = "bert_crf_model.pth"

# Load tr·ªçng s·ªë n·∫øu c√≥
if os.path.exists(model_path):
    model.load_state_dict(torch.load(model_path, map_location=device))
    print("‚úÖ ƒê√£ load m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán tr∆∞·ªõc ƒë√≥.")
else:
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh. B·∫°n c·∫ßn train t·ª´ ƒë·∫ßu.")
    # N·∫øu kh√¥ng c√≥, b·∫°n n√™n kh·ªüi t·∫°o model m·ªõi t·∫°i ƒë√¢y (ho·∫∑c b√°o l·ªói)

# Ti·∫øp t·ª•c hu·∫•n luy·ªán
batch_size = 8
train_dataset = TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BERTCRFModel("bert-base-uncased", num_labels=len(label2id)).to(device)

model_path = "bert_crf_model.pth"

# N·∫øu model ƒë√£ t·ªìn t·∫°i => Load
if os.path.exists(model_path):
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    print("‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i t·ª´ file. B·ªè qua b∆∞·ªõc hu·∫•n luy·ªán.")
else:
    print("üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh...")
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    epochs = 20

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dataloader:
            batch = [b.to(device) for b in batch]
            input_ids, attention_mask, labels = batch

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader):.4f}")

    torch.save(model.state_dict(), model_path)
    print("‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán v√† l∆∞u th√†nh c√¥ng!")

# Prediction Function
def predict(query):
    model.eval()
    input_ids, attention_mask, _ = encode_query(query, ["O"] * len(query.split()))
    input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)

    with torch.no_grad():
        predictions = model(input_ids, attention_mask)

    predicted_labels = [id2label[label] for label in predictions[0]]
    return list(zip(query.split(), predicted_labels))
app = Flask(__name__)

@app.route("/")
def home():
    return jsonify({"message": "Welcome to BERT-CRF NER API using Flask"})

@app.route("/predict", methods=["POST"])
def predict_api():
    data = request.get_json()
    query = data.get("query", "").strip()

    if not query:
        return jsonify({"error": "Query is required"}), 400

    result = predict(query)
    return jsonify({"entities": result})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)

# Example Prediction
query = "g√† ri r√°n KFC khuy·∫øn m√£i"
print(predict(query))

